<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Jane Doe">
      
      
      
      
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.22">
    
    
      
        <title>Investigating the GPU Neural Accelerators on Apple A19/M5</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.84d31ad4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#apple-gpu-architecture" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="Investigating the GPU Neural Accelerators on Apple A19/M5" class="md-header__button md-logo" aria-label="Investigating the GPU Neural Accelerators on Apple A19/M5" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Investigating the GPU Neural Accelerators on Apple A19/M5
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Investigating the GPU Neural Accelerators on Apple A19/M5
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/tzakharko/apple-neural-accelerators-benchmark" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    View on GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="Investigating the GPU Neural Accelerators on Apple A19/M5" class="md-nav__button md-logo" aria-label="Investigating the GPU Neural Accelerators on Apple A19/M5" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"></path></svg>

    </a>
    Investigating the GPU Neural Accelerators on Apple A19/M5
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/tzakharko/apple-neural-accelerators-benchmark" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg>
  </div>
  <div class="md-source__repository">
    View on GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Investigating the GPU Neural Accelerators on Apple A19/M5
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="." class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Investigating the GPU Neural Accelerators on Apple A19/M5
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#apple-gpu-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Apple GPU Architecture
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#neural-accelerators" class="md-nav__link">
    <span class="md-ellipsis">
      Neural Accelerators
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#experimental-setup" class="md-nav__link">
    <span class="md-ellipsis">
      Experimental Setup
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results-for-fp16" class="md-nav__link">
    <span class="md-ellipsis">
      Results for FP16
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results-for-int8" class="md-nav__link">
    <span class="md-ellipsis">
      Results for INT8
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimal-tile-size" class="md-nav__link">
    <span class="md-ellipsis">
      Optimal Tile Size
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusions-and-outlook" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusions and Outlook
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#apple-gpu-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Apple GPU Architecture
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#neural-accelerators" class="md-nav__link">
    <span class="md-ellipsis">
      Neural Accelerators
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#experimental-setup" class="md-nav__link">
    <span class="md-ellipsis">
      Experimental Setup
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results-for-fp16" class="md-nav__link">
    <span class="md-ellipsis">
      Results for FP16
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results-for-int8" class="md-nav__link">
    <span class="md-ellipsis">
      Results for INT8
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimal-tile-size" class="md-nav__link">
    <span class="md-ellipsis">
      Optimal Tile Size
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusions-and-outlook" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusions and Outlook
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  
  




  <h1>Investigating the GPU Neural Accelerators on Apple A19/M5</h1>

<p><strong>Key Takeaways:</strong></p>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Measured Peak Performance (A19, 5 GPU cores)</th>
<th>Speedup over previous generation (iso-clock)</th>
<th>Estimated M5 Max (40 cores)</th>
</tr>
</thead>
<tbody>
<tr>
<td>SIMD FP32</td>
<td>1880 GFLOPS</td>
<td>1</td>
<td>16 TFLOPS</td>
</tr>
<tr>
<td>SIMD FP16</td>
<td>3200 GFLOPS</td>
<td>~0.7</td>
<td>27 TFLOPS</td>
</tr>
<tr>
<td>Matrix FP16 (accumulating into FP16/FP32)</td>
<td>7500 GFLOPS</td>
<td>~4x</td>
<td>63 TFLOPS</td>
</tr>
<tr>
<td>Matrix INT8 (accumulating into INT32)</td>
<td>13500 GFLOPS</td>
<td>~7.5x</td>
<td>113 TFLOPS</td>
</tr>
</tbody>
</table>
<ul>
<li>Estimated 1024 FLOPS per GPU core per cycle for FP16 matrix-multiply accumulate</li>
<li>Estimated &lt;~2048 OPS per GPU core per cycle for INT8 (<em>with caveats</em>)</li>
<li>Optimal matrix size appears to be 32 <span class="arithmatex">\(\times\)</span> 32  or larger for all tested data formats</li>
</ul>
<div class="admonition note">
<p class="admonition-title">A note on terminology</p>
<p>GPU matrix acceleration technology was first popularized by Nvidia, which dubbed its accelerator "Tensor Cores". Since then, the term became a colloquial reference for any kind of GPU matrix multiplication acceleration hardware. For the sake of clarity, I will talk about "matrix multiplication units" in general or "Neural Accelerators" to refer specifically to Apple's implementation.</p>
<p>Another common point of confusion is how GPU specs are advertised. Most GPU vendors count the number of individual scalar FP32 arithmetic pipelines (e.g., "CUDA cores", "shader units", or similar). Apple instead uses "GPU cores", which refer to minimal replicable building blocks. A single Apple GPU core contains 128 FP32 "shader cores" and is roughly comparable to a "Streaming Multiprocessor" (SM) in an Nvidia Blackwell GPU. Similarly, the GPU performance is typically advertised as the theoretically attainable peak throughput using FP32 multiply-add operations. Real-world performance is often substantially different (usually lower) and depends on the workload and the details of the specific GPU architecture.</p>
</div>
<h2 id="apple-gpu-architecture">Apple GPU Architecture</h2>
<p>Modern GPUs are massively parallel processors that rely on wide SIMD (single instruction, multiple data) execution to maximize the compute density, and Apple GPUs are no exception. The native execution width of Apple hardware is 32, with arithmetic instructions operating on 32 data elements in parallel. The programming model presents this as multiple threads running the same shader code but potentially different data, with groups of 32 threads executing in lockstep and able to communicate (e.g., read each other's data) directly. Apple Metal refers to these execution groups as SIMD-groups (other mainstream designations include "wavefronts" and "warps"). Multiple SIMD-groups can be organized into threadgroups that have access to fast shared memory but require explicit synchronization. Finally, multiple threadgroups can be launched as part of the same compute kernel, but cannot directly synchronize their work and need to communicate using global memory and atomic memory operations.</p>
<p>This logical organization is directly reflected in the hardware organization (Figure 1). The basic building block is a compute partition, responsible for executing shader programs. Each compute partition consists of an instruction dispatch circuitry, execution circuitry (SIMD units), register cache, and additional supporting hardware and state (e.g., raytracing and texture sampling, omitted here for brevity). The execution circuitry consists of multiple physically distinct execution SIMD datapaths that handle instructions of different types. Experimental evidence and Apple WWDC presentations suggest that these include at least a full-precision floating point FMA pipe, a half-precision floating point FMA pipe, and an integer ADD pipe, plus potentially others. Blocks of four compute partitions are organized into GPU cores and share the fast local cache memory. A threadgroup is executed on a single GPU core, using the four compute partitions to provide within-threadgroup parallelism. Furthermore, each GPU core will execute instructions from several different threadgroups concurrently, switching between threads every cycle to maximize hardware occupancy and hide execution latencies. Therefore, the GPU relies on constantly being fed work in order to maintain high execution efficiency.</p>
<p>From the compute perspective, each partition therefore contains 32 FP32 multiply-accumulate units and is capable of performing 64 FLOPS per cycle (32 combined multiplications+additions). For a GPU core, this means 128 FP32 FMA units or 256 FLOPS per cycle. Note that this is only a theoretical maximum for FP32 performance, and actual shader performance can differ significantly depending on the instruction mix and memory access patterns. With newer Apple GPUs, the achievable performance can also occasionally be higher due to the ability to issue multiple instructions simultaneously (see below).</p>
<p>While the basic GPU core organization remained unchanged at least since A14/M1 (perhaps even earlier), there have been significant changes to the compute units and memory hierarchy over the years. Apple Family 9 GPUs (featured in A17 Pro/M3/M4 series of chips) introduced two major improvements to shader execution. The first is the ability to issue two instructions per clock to different datapaths from a single compute partition. For example, full-precision (FP32) instructions can be issued simultaneously with half-precision (FP16) or integer (INT) instructions, effectively improving runtime performance. The second is the introduction of dynamic shader memory management (which Apple dubs "Dynamic Caching") that allows all memory resources, including registers, shared memory, texture memory, and more, to be backed by the L2 cache and allocated dynamically. This feature can improve hardware occupancy for complex shaders that have different resource needs depending on some runtime parameter (e.g., a branch condition). For more information, refer to the <a href="https://developer.apple.com/videos/play/tech-talks/111375/">Explore GPU advancements in M3 and A17 Pro (Apple Tech Talk)</a>.</p>
<p>Apple A19/M5 GPUs introduce additional major changes to the compute units. Here is an incomplete list of new features I know of:</p>
<ul>
<li>New matric multiplication datapath with support for FP16 and INT8 data formats (Neural Accelerators)</li>
<li>Two half-precision (FP16) instructions can be issued concurrently, doubling the FP16 compute</li>
<li>The throughput of INT32 multiplication has been doubled from earlier</li>
<li>The throughput of some special operations (<code>log</code>/<code>exp</code>/<code>popcnt</code>) has been doubled</li>
</ul>
<p>At the same time, the clock frequency of the GPU has only moderately increased over the years. The original M1 GPU was clocked at around 1300 MHz, while I estimate the A19 GPU frequency to be around 1460 MHz (and the M5 will probably be around 1500 - 1600 MHz)<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>. This means that the majority of the substantial performance improvements stem from architectural changes and increased execution efficiency.</p>
<h2 id="neural-accelerators">Neural Accelerators</h2>
<p>Apple does not disclose any information about the architecture of the Neural Accelerator hardware. Some information might be inferred from the patents published in 2025:</p>
<ul>
<li><a href="https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2025071810">Matrix Multiplier Caching (Patent  WO2025071810)</a></li>
<li><a href="https://patentscope.wipo.int/search/en/detail.jsf?docId=US453093419">Operand Selection Circuitry (Patent US20250110734)</a></li>
</ul>
<p>In particular, <a href="https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2025071810">Fig. 2 and 3</a> from the WO2025071810 patent describe a dot product circuit that operates on four pairs of values per data lane, and the other patent describes a routing network for efficiently transposing matrix elements in order to supply such a circuit with data. The benchmark results are consistent with the idea of a 32-wide 4-way hardware dot product datapath that has an effective throughput of 128 half-precision multiply-accumulate operations each cycle. The dot product throughput for each GPU core is therefore 512 FP16 FMAs per cycle or 1024 FLOPS per cycle.</p>
<p>Regardless of the implementation details, the matrix multiplication hardware is not directly exposed to the developers writing Metal Shading Language kernels. As of Xcode 26.1, the only supported way to access it is by using the high-level Metal Performance Primitives (MPP) framework in combination with the new Metal Tensor APIs. These frameworks provide C++ templates for configuring and executing accelerated operations on tensors, currently including matrix multiplication and convolution. The matrix multiplication operation is performed on matrices of dimensions <span class="arithmatex">\(M \times K\)</span> and <span class="arithmatex">\(K \times N\)</span>, producing a matrix of dimensions <span class="arithmatex">\(M \times N\)</span><sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>. Compile-time parameters can be used to configure the behavior of matrix multiplication; an overview is provided in Table 1.</p>
<figure id="__table-caption_1">
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Explanation</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Matrix dimensions</td>
<td>Constants <span class="arithmatex">\(M\)</span>, <span class="arithmatex">\(N\)</span>, and <span class="arithmatex">\(K\)</span></td>
<td><em><span class="arithmatex">\(K\)</span> can be inferred at runtime from the tensor shape</em></td>
</tr>
<tr>
<td>Matrix transpose</td>
<td>Flags indicating whether to transpose the input matrices</td>
<td></td>
</tr>
<tr>
<td>Precision</td>
<td>Flag indicating whether reduced precision is acceptable</td>
<td><em>Appears to have no effect on A19</em></td>
</tr>
<tr>
<td>Operation type</td>
<td>Multiply with or without accumulation</td>
<td><em>Accumulation updates the output tensor in place</em></td>
</tr>
<tr>
<td>Execution scope</td>
<td>Thread, SIMD-Group, or multiple SIMD-Groups</td>
<td><em>A single SIMD-group appears to work best on current implementation</em></td>
</tr>
</tbody>
</table>
<figcaption>
<p><span class="caption-prefix">Table 1.</span> MPP Matrix Multiplication Configuration as Compile-Time Constants</p>
</figcaption>
</figure>
<p>Configurable execution scope is a particularly interesting feature, as it allows the programmer to choose between different execution modes. There is a thread-scope, where each thread performs its own matrix multiplication — probably most useful for small matrices with divergent execution flow, e.g., for graphical tasks. The SIMD-groups scope is a classical hardware-accelerated cooperative matrix multiplication mode, where all threads in a compute partition concurrently work towards producing the result. This scope can also be configured to involve multiple parallel SIMD-groups, presumably pooling the hardware resources of several partitions, but in my experiments, this always resulted in a significant performance drop. This will likely be fixed in a future update.</p>
<p>The MPP framework requires the output matrix size (tile size) to be fixed at compile time by providing the constants <span class="arithmatex">\(M\)</span> and <span class="arithmatex">\(N\)</span>. The remaining input matrix dimension can be either a constant or determined dynamically from the input tensors' shape. While the framework is quite flexible in setting up the dimensions, it seems that only a few choices work well in practice. Using too small or too large matrices generally resulted in a poor performance or a shader compilation error. Some dimensions produced wrong results (which appears to be a bug with masking out unused lanes in the dot product hardware). Performance figures for different tile sizes are presented further down.</p>
<p>The framework supports several data types as inputs and outputs for matrix operations. Hardware-accelerated data formats include FP16 (with FP16 or FP32 output) and INT8 (with INT32 output). Although undocumented, FP32 is supported as well, but appears to run using the general-purpose SIMD pipe on the A19. A notable omission is the Bfloat16 format — it is unclear whether the first-generation Neural Accelerator hardware lacks dedicated support for this format or whether it is not yet exposed in the Metal Shading Language.</p>
<p>Finally, the output of the matrix multiplication operation can be either a tensor object (usually stored in device memory) or a special <code>cooperative_tensor</code> object that partitions the tensor storage among all participating threads in an implementation-defined way (this is similar to "matrix fragments" in Nvidia PTX). Cooperative tensors are useful when performing multiple operations in a sequence or post-processing the output, since the data resides in local registers and is very fast to access. Perhaps surprisingly, cooperative tensors currently cannot be used as inputs for matrix multiplication.</p>
<h2 id="experimental-setup">Experimental Setup</h2>
<p>To measure the peak performance of Neural Accelerators, I have set up a simple testing framework using Swift and Metal Shading Language. The code is available in the project repository. A helper function is used to generate and build specialized matrix multiplication kernels for the given matrix dimensions and data types. We then launch a very large number of threadgroups and measure the elapsed time. Since we are interested in peak performance, special care is taken to ensure that memory access overhead is negligible:</p>
<ul>
<li>We use the same inputs for each kernel, ensuring that the data will be in the fast GPU-local cache instead of the slow device memory</li>
<li>We repeat the multiply-accumulate operation multiple times (64 repetitions) and use a cooperative tensor to ensure high performance</li>
</ul>
<p>For given matrix dimensions <span class="arithmatex">\(M\)</span>, <span class="arithmatex">\(K\)</span>, and <span class="arithmatex">\(N\)</span> and the elapsed time <span class="arithmatex">\(t\)</span>, we can calculate the kernel throughput as follows<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup>:</p>
<div class="arithmatex">\[
\text{OPS} = \frac{64\times(2MNK)}{t}
\]</div>
<p>These tests are performed for the supported input data types (FP16 and INT8) with supported accumulator types. We also test a variety of matrix dimensions for <span class="arithmatex">\(M, N \in\)</span> {8, 16, 32} and <span class="arithmatex">\(K \in\)</span> {8, 16, 32, 64, 128}. As choosing certain matrix dimensions produces invalid results on A19, we also validate the kernel output by comparing it with a reference implementation. Invalid results are discarded. To account for variance in measurements, we repeat each test multiple times and take the highest measured value. The tests were performed on an A19 chip with 5 GPU cores (iPhone 17), with reference data provided using a M3 Max MacBook Pro and a M4 iPad Pro. The A19 tests were performed outdoors during a particularly cold Swiss autumn evening to help with the thermal performance, which might have been more of a placebo than not.</p>
<h2 id="results-for-fp16">Results for FP16</h2>
<p>MPP supports matrix multiplication for FP16 inputs using a FP16 or a FP32 accumulator matrix. The performance remains the same for both accumulator types. This is in contrast to some other implementations, which offer higher performance when a FP16 accumulator is used (e.g., Nvidia Tensor Cores). The performance is highly dependent on the matrix dimensions (see the following sections for a more detailed discussion). The results are summarized in Figure 1 below. The output matrix (tile) dimensions are <span class="arithmatex">\(M\)</span> (rows) and <span class="arithmatex">\(N\)</span> (columns). The number <span class="arithmatex">\(K\)</span> is the other dimension of the input matrices (columns for the first matrix and rows for the second matrix). We only show the results for the FP32 accumulator since they are identical to the FP16 accumulator.</p>
<figure id="__figure-caption_1">
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="assets/mma_fp16_fp32.svg" data-desc-position="bottom"><img alt="Image title" src="assets/mma_fp16_fp32.svg"></a></p>
<figcaption>
<p><span class="caption-prefix">Figure 1.</span> A19 Neural Accelerator Performance for FP16 Matrix Multiplication with FP32 Accumulator</p>
</figcaption>
</figure>
<p>The peak measured throughput is approximately 7.4 TFLOPS for the 5-core A19 smartphone chip. Given the estimated GPU clock frequency of 1460 MHz, each core provides around 1000 FLOPS of matrix multiplication throughput per cycle. This number is very close to 1024 FLOPS, or 128 dot product FMA operations per cycle per compute partition. As mentioned previously, this is very similar to the 4-way 32-wide dot product accelerator described in a published patent, and too close a number to be a coincidence. Even if the hardware implementation details differ, I believe it is safe to conclude that we have <span class="arithmatex">\(128\)</span> matrix FMAs per partition.</p>
<h2 id="results-for-int8">Results for INT8</h2>
<p>MPP supports matrix multiplication for INT8 inputs using a FP32 accumulator matrix. As previously mentioned, the performance is highly dependent on the matrix dimensions. The results are summarized in Figure 2 below. The output matrix (tile) dimensions are <span class="arithmatex">\(M\)</span> (rows) and <span class="arithmatex">\(N\)</span> (columns). The number <span class="arithmatex">\(K\)</span> is the other dimension of the input matrices (columns for the first matrix and rows for the second matrix).</p>
<figure id="__figure-caption_2">
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="assets/mma_i8_i32.svg" data-desc-position="bottom"><img alt="Image title" src="assets/mma_i8_i32.svg"></a></p>
<figcaption>
<p><span class="caption-prefix">Figure 2.</span> A19 Neural Accelerator Performance for INT8 Matrix Multiplication with INT32 Accumulator</p>
</figcaption>
</figure>
<p>The peak measured throughput is approximately 13.4 TOPS, just short of a double compared to the FP16 figures. Given the estimated GPU clock frequency of 1460 MHz, this translates to approximately 1900 OPS per GPU core per cycle. Unlike before, this is not a very nice power of two number, but it is sufficiently close to 2048, which is a power of two. It would be convenient to assume 2048 OPS per core per cycle, since that would translate to a plausible 256 dot product MAC (integer multiply and accumulate) — double of FP16 compute. Still, I am a bit uneasy about making this jump given the larger discrepancy between this plausible value and the measured value. It is possible that the GPU throttles slightly while running the Neural Accelerator hardware in INT mode, which would explain a slightly lower performance. I did obtain a measurement of over 14 TOPS during some preliminary benchmark runs, but was not able to replicate it using the final version of the benchmark suite. At any rate, we do observe a noticeable improvement over FP16 implementation, suggesting that Apple has actually undersold the capability of these chips in their marketing materials.</p>
<h2 id="optimal-tile-size">Optimal Tile Size</h2>
<p>The experimental results show that the matrix multiplication performance highly depends on the chosen matrix tile size. It appears that the optimal matrix size for both FP16 and INT8 data is around 32 <span class="arithmatex">\(\times\)</span> 32 for both input matrices. Doubling the matrix size (not shown in the graphs) appears to decrease the performance again, and increasing the dimension beyond certain limits seems to crash the Metal compiler on the A19 target.</p>
<p>With 128 presumed matrix FMAs per partition, 256 cycles are required in total to compute the matrix product for <span class="arithmatex">\(M=N=K=\)</span>32, which seems like a surprisingly high number of cycles just to hide the latency. As Apple does not provide any architectural or performance tuning details at this moment, I find it difficult to speculate on the exact reason. It might be a latency issue, or maybe we are observing the operand routing network in action, which could require a specific data layout to perform well. Regardless, pending any official guidance from Apple, I would go for at least 32 <span class="arithmatex">\(\times\)</span> 32 or even 32 <span class="arithmatex">\(\times\)</span> 64 matrix chunks.</p>
<h2 id="conclusions-and-outlook">Conclusions and Outlook</h2>
<p>While not without limitations, the first generation of Neural Accelerators is a significant step forward for programmable ML applications on Apple Silicon. We observe:</p>
<ul>
<li>1024 FLOPS per GPU core per cycle for FP16 matrix operations</li>
<li>~2048 OPS per GPU core per cycle for INT8 matrix operations</li>
<li>Optimal matrix tile size of approximately 32 <span class="arithmatex">\(\times\)</span> 32</li>
</ul>
<p>In particular, I'd like to highlight the strong FP16 performance with FP32 accumulators, which compares favorably with the current state of the art implementations. On the other hand, INT8 performance lags behind, and support for BFloat16 and additional reduced-precision ML-optimized data types is lacking. Regardless, the performance improvements are significant (the A19 iPhone GPU matches my M3 Max MacBook Pro for FP16  matrix multiplication!), which should make M5-based machines much more attractive for ML applications, especially when paired with larger unified memory pools than other consumer systems.</p>
<p>The results presented here focused on hardware capability and peak performance. Real-world applications are more complex because they also need to move data to and from the device memory. Based on simple napkin math, the required data bandwidth far exceeds the capabilities of current memory interfaces. A single half-precision 32 <span class="arithmatex">\(\times\)</span> 32 matrix is 2KB of data, so 4KB needs to be fetched per compute partition every 256 cycles to keep the Neural Accelerators fully utilized.  That's 16 bytes per cycle per partition or 64 bytes per cycle for a single GPU core. At the GPU clock frequency of 1460 MHz, this translates to 93.44 GB/s inbound bandwidth per GPU core. In comparison, the new M5 Mac has ~ 150 GB/s for 10 GPU cores. State-of-the-art algorithms for Nvidia hardware rely on sophisticated data prefetching and caching strategies. It would be interesting to explore how these ideas can be adapted to the Apple Silicon architecture and the new Metal 4 Tensor APIs.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>The frequency was estimated by measuring the maximal achievable throughput of an FMA chain kernel, which tops out at around <span class="arithmatex">\(1880 GFLOPS\)</span>. With <span class="arithmatex">\(128\)</span> FMA pipelines per core (<span class="arithmatex">\(640\)</span> pipelines in total) and two FLOPS per pipeline, this results in <span class="arithmatex">\(1880/(640*2) \approx 1.46\)</span> GHz.&nbsp;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:2">
<p>Metal stores matrices in row-major order, which means that the matrix dimensions are reversed for a tensor. The first dimension specifies the number of columns (elements per row), and the second dimension specifies the number of rows. This does not appear to be currently documented and can be somewhat confusing.&nbsp;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
<li id="fn:3">
<p>Each matrix multiplication needs <span class="arithmatex">\(MNK\)</span> FMA operations, and an FMA counts as two operations (multiplication and addition). We repeat this 64 times.&nbsp;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">↩</a></p>
</li>
</ol>
</div>







  
  




  



                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Taras Zakharko — Published October 15, 2025
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": ".", "features": ["navigation.instant", "content.code.copy", "content.tabs.link", "navigation.footer"], "search": "assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="javascripts/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>